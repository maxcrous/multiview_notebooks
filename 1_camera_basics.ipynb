{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b25b59b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3D Visualization of calibrated cameras and a chessboard\n",
    "\n",
    "This notebook visualizes the basics of perspective projection and pinhole cameras.    \n",
    "\n",
    "**Subjects covered** \n",
    "\n",
    "1. **Camera calibration** using a chessboard and OpenCV's calibration functions. \n",
    "2. **Interchanging coordinate frames**  \n",
    "    That is, switching between: \n",
    "    * *3D world coordinate frame* - a frame with standard basis vectors centered at the world origin.\n",
    "    * *2D camera image plane* - a frame centered at a camera's projection center and rotated    \n",
    "         to align the $z$-axis with the camera's principal axis, and the $x$ and $y$ axis with the camera's image plane. \n",
    "    * *2D camera pixel plane* - another camera-centric coordinate frame, but with appropriate scaling, skewing,    \n",
    "        and translating to align coordinates of projected points with the camera's pixel coordinates of those points. \n",
    "3. **Visualizing camera poses and a chessboard in 3D**.  \n",
    "4. **Superimposing an object into a scene**.   \n",
    "5. **Back-projecting image points** to exhibit the ray along which corresponding 3D object points will lie. \n",
    "6. **Triangulating 3D world points** by finding the (near-)intersection of two rays of back-projected SIFT correspondences.\n",
    "7. **Conclusion**\n",
    "7. **Sources**\n",
    "\n",
    "**Why are we interested in these subjects?**    \n",
    "Real-world cameras can be set up to approximate a pinhole camera. Pinhole camera image formation is well modeled by the maths of perspective projection.    \n",
    "This 2D image$\\leftrightarrow$3D world correspondence enables us to infer world structure from images, and images from world structure.   \n",
    "This gives rise to a range of useful applications, including: \n",
    "* measuring objects in the scene (photogrammetry) \n",
    "* collision avoidance for autonomous robots (depth perception)\n",
    "* inserting objects into a scene (virtual reality)\n",
    "* superimposing textures onto a scene (virtual reality)\n",
    "* creating 3D models of real-world objects (metric reconstruction).\n",
    "\n",
    "**Prerequisites**    \n",
    "A basic familiarity with linear algebra, Python, and the camera matrix is assumed. For the camera matrix, reading [this OpenCV page](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html ) should suffice. \n",
    "\n",
    "**Further reading**   \n",
    "For a comprehensive treatment of perspective projection, see the first four chapters of [Multiple view geometry in computer vision](https://www.robots.ox.ac.uk/~vgg/hzbook/) by Richard Hartley and Andrew Zisserman. For a less thorough but more merciful introduction, watch [the second lecture](https://www.youtube.com/watch?v=iL4WhR0hzns&list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&ab_channel=T%C3%BCbingenMachineLearning) of Andreas Geiger's computer vision course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e61ad9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b8983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce3c96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calibrate cameras\n",
    "[Camera calibration](https://www.mathworks.com/help/vision/ug/camera-calibration.html) refers to the process of determining the extrinsic and intrinsic parameters of a camera. Extrinsic parameters model the pose of a camera in 3D space. Viewed as a transformation, the extrinsic matrix maps 3D points to the camera's image plane. Intrinsic parameters model how these 2D points in the image plane relate to the 2D pixel coordinates in the camera's image. Without knowing the intrinsics, one cannot infer object size, angles, parallelism, and other useful scene properties. Now we will see how to retrieve the intrinsic and extrinsic parameters.\n",
    "\n",
    "As the camera extrinsics and instrinsics encode a 2D pixel$\\leftrightarrow$3D world relation, knowing 2D$\\leftrightarrow$3D correspondences lets us compute the parameters. To get 2D$\\leftrightarrow$3D correspondences, a calibration object with known geometry such as a chessboard can be used. The OpenCV library has a function [findChessboardCorners](https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a) that supports a chessboard setup. This means we do not need to manually label pixels to find 2D pixel$\\leftrightarrow$3D world correspondences. To then calculate the intrinsics from correspondences, OpenCV's [calibrateCamera function](https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d) is used, which implements [Zhang's camera calibration algorithm](https://ieeexplore.ieee.org/abstract/document/888718/). For a high-level summary of this algorithm, see the appendix at the end of this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc7b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calibrate_cameras() :\n",
    "    \"\"\" Calibrates cameras from chessboard images. \n",
    "    \n",
    "    Returns: \n",
    "        images (list[np.ndarray]): Images containing the chessboard.\n",
    "        intrinsics (np.ndarray): An upper triangular 4x4 full-rank matrix containing camera intrinsics.\n",
    "        distortions (np.ndarray): Radial distortion coefficients.\n",
    "        rotation_vectors (list[np.ndarray]): Rodrigues rotation vectors.\n",
    "        translation_vectors (list[np.ndarray]): Translation vectors.\n",
    "        object_points: (np.ndarray): A (4, 54) point array, representing the [x,y,z,w]\n",
    "            of 54 chessboard points (homogenous coordiantes).\n",
    "    \"\"\"\n",
    "    images = list() \n",
    "    \n",
    "    # Read images\n",
    "    for i in range(11): \n",
    "        img = cv2.imread(f'./images/{i}.jpg')\n",
    "        img = cv2.resize(img, None, fx=0.25, fy=0.25)\n",
    "        images.append(img) \n",
    "\n",
    "    # The default opencv chessboard has 6 rows, 9 columns \n",
    "    shape = (6, 9) \n",
    "\n",
    "    # List to store vectors of 3D world points for every checkerboard image\n",
    "    object_points_all = []\n",
    "    \n",
    "    # List to store vectors of 2D projected points for every checkerboard image\n",
    "    image_points_all = [] \n",
    "    \n",
    "    # Flags for chessboard corner search. Taken from opencv docs.\n",
    "    flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\n",
    "    \n",
    "    # Criteria for termination of the iterative corner refinement. Taken from opencv docs.\n",
    "    refinement_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "    # List for images in which chessboard is be found by search\n",
    "    images_filtered = list()\n",
    "    \n",
    "    # Object points in a single image. Simply a row iterated list of z=0 3d points. \n",
    "    # E.g. [[0. 0. 0.] [1. 0. 0.] ... [0, 1, 0], [1, 1, 0], ... ]\n",
    "    object_points = np.zeros((1, shape[0] * shape[1], 3), np.float32)\n",
    "    object_points[0, :, :2] = np.mgrid[0:shape[0], 0:shape[1]].T.reshape(-1, 2)\n",
    "    \n",
    "    # For each image, store the object points and image points of chessboard corners.\n",
    "    for idx, image in enumerate(images): \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        succes, corners = cv2.findChessboardCorners(image=gray, \n",
    "                                                    patternSize=shape,\n",
    "                                                    flags=flags) \n",
    "        if succes:\n",
    "            images_filtered.append(image)\n",
    "            corners = cv2.cornerSubPix(image=gray, \n",
    "                                       corners=corners, \n",
    "                                       winSize=(11,11), \n",
    "                                       zeroZone=(-1,-1), \n",
    "                                       criteria=refinement_criteria)\n",
    "            object_points_all.append(object_points)\n",
    "            image_points_all.append(corners)    \n",
    "            \n",
    "    images = images_filtered\n",
    "    \n",
    "    # Calibrate the cameras by using the 3D <-> 2D point correspondences.\n",
    "    ret, intrinsics, distortions, rotation_vectors, translation_vectors = cv2.calibrateCamera(object_points_all, image_points_all, gray.shape[::-1], None, None)\n",
    "    \n",
    "    # Make intrinsic matrix 4x4 full-rank to ease manipulation.\n",
    "    intrinsics = np.hstack([intrinsics, np.zeros((3, 1))])\n",
    "    intrinsics = np.vstack([intrinsics, [[0, 0, 0, 1]]])\n",
    "\n",
    "    # Convert chessboard object points to homogeneous coordinates to ease later use.\n",
    "    object_points = object_points[0].reshape((-1, 3)).T\n",
    "    object_points = np.vstack([object_points, np.ones((1, object_points.shape[1]))])\n",
    "    \n",
    "    return images, intrinsics, distortions, rotation_vectors, translation_vectors, object_points\n",
    "    \n",
    "images, intrinsics, distortions, rotation_vectors, translation_vectors, object_points = calibrate_cameras()\n",
    "\n",
    "print(f'\\n\\nNumber of calibration images: {len(images)}')\n",
    "print(f'Number of calibration points: {object_points.shape[1]}')\n",
    "print(f'\\n\\nSample of imags used')\n",
    "plt.figure(figsize=(20,20))\n",
    "_ = plt.imshow(cv2.hconcat([cv2.cvtColor(im, cv2.COLOR_RGB2BGR) for im in images[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab36ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert calibration output to extrinsic matrices\n",
    "The calibration method returns regular translations and rotations encoded as [Rodrigues rotation](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula) vectors. After converting these rotation vectors into rotation matrices we can form $3 \\times 4$ camera extrinsic matrices $[\\mathbf{R}| \\mathbf{t}] = \\mathbf{E}$. Matrix multiplying the extrinsic matrix with a homogenous 3D point returns the point projected to the camera's image plane (a camera-centeric coordinate frame where $z=1$).  \n",
    "\n",
    "$$ \\text{point in camera image plane} = [\\mathbf{R}| \\mathbf{t}] \\cdot \\text{point in world ref.}$$\n",
    "\n",
    "The camera projection center is the origin of this camera-centric coordinate frame. This is the point mapped to zero by the extrinsics, i.e. the extrinsics' null space. Keep in mind that $\\mathbf{EC} = \\mathbf{RC} + \\mathbf{T}$, that is, applying the extrinsic matrix to a homogenous point is the same as applying the rotation and adding a translation separately.\n",
    "\n",
    "$$ \\begin{align}\n",
    " 0 &= \\mathbf{EC}\\\\\n",
    " 0 &= \\mathbf{RC} + \\mathbf{T}\\\\\n",
    "-\\mathbf{RC} &= \\mathbf{T}\\\\\n",
    "-\\mathbf{R^T} \\cdot -\\mathbf{RC} &= -\\mathbf{R^T}\\mathbf{T}\\\\\n",
    " \\mathbf{C} &= -\\mathbf{R^T}\\mathbf{T}\n",
    " \\end{align}$$\n",
    " \n",
    "This formula will now be used to infer the camera centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441c44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extrinsics_from_calibration(rotation_vectors, translation_vectors):\n",
    "    \"\"\" Calculates extrinsic matrices from calibration output. \n",
    "        \n",
    "    Args: \n",
    "        rotation_vectors (list[np.ndarray]): Rodrigues rotation vectors.\n",
    "        translation_vectors (list[np.ndarray]): Translation vectors.\n",
    "    Returns: \n",
    "        extrinsics (list[np.ndarray]): A list of camera extrinsic matrices.\n",
    "            These matrices are 4x4 full-rank.\n",
    "    \"\"\"\n",
    "    \n",
    "    rotation_matrices = list() \n",
    "    for rot in rotation_vectors:\n",
    "        rotation_matrices.append(cv2.Rodrigues(rot)[0]) \n",
    "\n",
    "    extrinsics = list()\n",
    "    for rot, trans in zip(rotation_matrices, translation_vectors): \n",
    "        extrinsic = np.concatenate([rot, trans], axis=1)\n",
    "        extrinsic = np.vstack([extrinsic, [[0,0,0,1]]]) \n",
    "        extrinsics.append(extrinsic)\n",
    "    \n",
    "    return extrinsics\n",
    "\n",
    "def camera_centers_from_extrinsics(extrinsics):\n",
    "    \"\"\" Calculates camera centers from extrinsic matrices. \n",
    "    \n",
    "    Args: \n",
    "        extrinsics (list[np.ndarray]):  A list of camera extrinsic matrices.\n",
    "    Returns: \n",
    "        camera_centers (list[np.ndarray]): Homogenous coordinates of camera centers in \n",
    "            3D world coordinate frame.\n",
    "    \"\"\"\n",
    "    camera_centers = list() \n",
    "\n",
    "    for extrinsic in extrinsics:\n",
    "        rot = extrinsic[:3, :3]\n",
    "        trans = extrinsic[:3, 3]\n",
    "        center = -rot.T @ trans\n",
    "        center = np.append(center, 1)\n",
    "        camera_centers.append(center)\n",
    "    \n",
    "    return camera_centers\n",
    "\n",
    "extrinsics = extrinsics_from_calibration(rotation_vectors, translation_vectors)\n",
    "camera_centers = camera_centers_from_extrinsics(extrinsics)\n",
    "\n",
    "print(f'Shape of an extrinsic matrix: {extrinsics[0].shape}')\n",
    "print(f'Shape of the intrinsic matrix: {intrinsics.shape}')\n",
    "print(f'Number of cameras in the scene: {len(camera_centers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3b44a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invertible perspective projection \n",
    "Perspective projection of a pinhole camera is performed in two steps. First, the 3D point is projected to the camera's image plane using the extrinsic matrix. Then, the point on the image plane is scaled, skewed, and tranlsated to align with the camera's image pixel coordinates using the intrinsic matrix.\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\text{2D pixel point} &= \\text{Align with pixels} (\\text{Proj to cam image plane}(\\text{3D world point})) \\\\\n",
    "\\mathbf{x}&=\\mathbf{KEX} \\\\\n",
    "\\begin{bmatrix} x\\\\ y\\\\ w \\end{bmatrix} &= \\begin{bmatrix} f_{x} & \\alpha     & c_{x0} \\\\\n",
    "                                                            0    & f_{y} & c_{y0} \\\\\n",
    "                                                            0    & 0     & 1  \\end{bmatrix} \n",
    "                                           \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_{x} \\\\\n",
    "                                                           r_{24} & r_{25} & r_{26} & t_{y} \\\\\n",
    "                                                           r_{36} & r_{37} & r_{38} & t_{z} \\end{bmatrix}\n",
    "                                           \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ W \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "Often we refer to product $\\mathbf{KE}$ as the projection matrix $\\mathbf{P}$.  \n",
    "\n",
    "As one may notice, the extrinsic matrix is not square in this description, while in the code it is. The reason is that it's useful to be able to invert the projection matrix and its components, i.e. $\\mathbf{P^{-1}}, \\mathbf{E^{-1}}, \\mathbf{K^{-1}}$. This allows easy switching between coordinate frames using matrix multiplication. To achieve this, add the row $(0, 0, 0, 1)$ to make $\\mathbf{E}$ square and full-rank. Similarly, make $\\mathbf{K}$ full-rank $4\\times4$.\n",
    "  \n",
    "$$\\mathbf{\\tilde{P}} = \\begin{bmatrix} \\mathbf{K} &  0 \\\\ 0^{T} & 1 \\end{bmatrix}\n",
    "              \\begin{bmatrix} \\mathbf{R} &  t \\\\ 0^{T} & 1 \\end{bmatrix} = \\mathbf{\\tilde{K}\\tilde{E}}$$ \n",
    "              \n",
    "The 4x4 projection matrix $\\mathbf{\\tilde{P}}$ can now be used to map directly from 3D world coordinates $\\mathbf{\\bar{p}} = (x_w, y_w, z_w,1)$ to screen coordinates plus disparity, $\\mathbf{x_s} = (x_s, y_s, 1, d)$  \n",
    "\n",
    "$$ \\mathbf{x_s} \\sim \\mathbf{\\tilde{P}\\bar{p}_w} $$\n",
    "\n",
    "where $\\sim$ indicates equality up to scale. Note that after multiplication by $\\mathbf{\\tilde{P}}$, the vector must be divided by the third element of the vector to obtain this normalized form $\\mathbf{x_s} = (x_s, y_s, 1, d)$.    \n",
    "\n",
    "Whenever we don't want the full expressivity of a $4\\times4$ projection matrix we can always slice the intrinsics and extrincs back to their original size. E.g. when we do not want to specify a disparity.\n",
    "\n",
    "Partial excerpt taken from Computer Vision Algorithms and Applications by Richard Szeliski pg.49, section 'Camera matrix'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1cd6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot chessboard and cameras\n",
    "Given the extrinsics and a priori defined chessboard object points, a 3D visualization can be created. The [Ipyvolume](https://github.com/maartenbreddels/ipyvolume) library is used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d0b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cam_sphere_size = 1\n",
    "\n",
    "def init_3d_plot():\n",
    "    \"\"\" Initializes a ipyvolume 3d plot and centers the \n",
    "        world view around the center of the chessboard.\n",
    "        \n",
    "    Returns: \n",
    "        fig (ipyvolume.pylab.figure): A 3D plot. \n",
    "    \"\"\"\n",
    "    chessboard_x_center = 2.5\n",
    "    chessboard_y_center = 4\n",
    "    fig = ipv.pylab.figure(figsize=(15, 15), width=800)\n",
    "    ipv.xlim(2.5 - 30, 2.5 + 30)\n",
    "    ipv.ylim(4 - 30, 4 + 30)\n",
    "    ipv.zlim(-50, 10)\n",
    "    ipv.pylab.view(azimuth=40, elevation=-150)\n",
    "    return fig\n",
    "\n",
    "def plot_chessboard(object_points):\n",
    "    \"\"\" Plots a 3D chessboard and highlights the \n",
    "        objects points with green spheres. \n",
    "        \n",
    "    Args:  A (4, 54) point array, representing the [x,y,z,w]\n",
    "           of 54 chessboard points (homogenous coordiantes).\n",
    "    \"\"\"\n",
    "    img = cv2.imread('./images/chessboard.jpg')\n",
    "    img_height, img_width, _ = img.shape\n",
    "    chessboard_rows, chessboard_cols = 7, 10\n",
    "    xx, yy = np.meshgrid(np.linspace(0, chessboard_rows, img_height), \n",
    "                         np.linspace(0, chessboard_cols, img_width)) \n",
    "    zz = np.zeros_like(yy)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255\n",
    "    # -1 is used as the start of the board images x and y coord, \n",
    "    # such that the first inner corner appear as coord (0, 0, 0) \n",
    "    ipv.plot_surface(xx-1, yy-1, zz, color=cv2.transpose(img))\n",
    "    xs, ys, zs, _ = object_points\n",
    "    ipv.scatter(xs, ys, zs, size=1, marker='sphere', color='lime')\n",
    "    \n",
    "def plot_camera_axis(cam_center, inv_extrinsic, vis_scale):\n",
    "    \"\"\" Plots the x, y, and z basis vectors of the camera coordinate frame.\n",
    "        This is achieved by  transforming the basis vectors into the world \n",
    "        coordinate frame and plotting them. \n",
    "    \n",
    "    Args:\n",
    "        cam_center (np.ndarray): Coordinates of camera centers in \n",
    "            3D world coordinate frame.                                                                                                                                                   :\n",
    "        inv_extrinsic (np.ndarray): The (pseudo)-inverse of camera extrinsic matrix.\n",
    "        vis_scale (int): A visualization size multiplyer to make cameras \n",
    "            appear larger and easier to see.\n",
    "    \"\"\"\n",
    "    x, y, z, _ = cam_center\n",
    "    x_arrow = inv_extrinsic @ (1*vis_scale, 0, 0,  1)\n",
    "    y_arrow = inv_extrinsic @ (0, 1*vis_scale, 0,  1)\n",
    "    z_arrow = inv_extrinsic @ (0, 0, 1*vis_scale,  1)\n",
    "    \n",
    "    ipv.plot([x, x_arrow[0]], [y, x_arrow[1]], [z, x_arrow[2]], color='red')\n",
    "    ipv.plot([x, y_arrow[0]], [y, y_arrow[1]], [z, y_arrow[2]], color='blue')\n",
    "    ipv.plot([x, z_arrow[0]], [y, z_arrow[1]], [z, z_arrow[2]], color='green')\n",
    "    ipv.scatter(np.array([x]), np.array([y]), np.array([z]), size=cam_sphere_size, marker=\"sphere\", color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04f872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "\n",
    "# Determines the visual scale of the plotted cameras \n",
    "vis_scale = 5\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images): \n",
    "    height, width, _ = image.shape\n",
    "    inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "    plot_camera_axis(cam_center, inv_extrinsic, vis_scale)\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21ba29",
   "metadata": {},
   "source": [
    "## Plot cameras as viewport wireframes\n",
    "Instead of camera-centric coordinate frames, we can plot a slightly more intuitive wireframe of each camera's viewport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05e12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual dimension of the camera in the 3D plot. \n",
    "height, width, _ = images[0].shape\n",
    "camera_aspect_ratio = width / height\n",
    "# A length of 1 corresponds to the length of 1 chessboard cell.\n",
    "# This is because a chessboard points have been defined as such.\n",
    "# Set height of camera viewport to 1.\n",
    "vis_cam_height = 1 \n",
    "vis_cam_width = vis_cam_height * camera_aspect_ratio\n",
    "wire_frame_depth = 1.2\n",
    "\n",
    "def plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic):\n",
    "    \"\"\" Plots the wireframe of a camera's viewport. \"\"\"\n",
    "    x, y, z, _ = cam_center \n",
    "    \n",
    "    # Get left/right top/bottom wireframe coordinates\n",
    "    # Use the inverse of the camera's extrinsic matrix to convert \n",
    "    # coordinates relative to the camera to world coordinates.\n",
    "    lt = inv_extrinsic @ np.array((-vis_cam_width/2, -vis_cam_height/2, wire_frame_depth, 1/vis_scale)) * vis_scale\n",
    "    rt = inv_extrinsic @ np.array((vis_cam_width/2,  -vis_cam_height/2, wire_frame_depth, 1/vis_scale)) * vis_scale\n",
    "    lb = inv_extrinsic @ np.array((-vis_cam_width/2,  vis_cam_height/2, wire_frame_depth, 1/vis_scale)) * vis_scale\n",
    "    rb = inv_extrinsic @ np.array((vis_cam_width/2,   vis_cam_height/2, wire_frame_depth, 1/vis_scale)) * vis_scale\n",
    "\n",
    "    # Connect camera projective center to wireframe extremities\n",
    "    ipv.plot([x, lt[0]], [y, lt[1]], [z, lt[2]], color='blue')\n",
    "    ipv.plot([x, rt[0]], [y, rt[1]], [z, rt[2]], color='blue')\n",
    "    ipv.plot([x, lb[0]], [y, lb[1]], [z, lb[2]], color='blue')\n",
    "    ipv.plot([x, rb[0]], [y, rb[1]], [z, rb[2]], color='blue')\n",
    "    \n",
    "    # Connect wireframe corners with a rectangle\n",
    "    ipv.plot([lt[0], rt[0]], [lt[1], rt[1]], [lt[2], rt[2]], color='blue')\n",
    "    ipv.plot([rt[0], rb[0]], [rt[1], rb[1]], [rt[2], rb[2]], color='blue')\n",
    "    ipv.plot([rb[0], lb[0]], [rb[1], lb[1]], [rb[2], lb[2]], color='blue')\n",
    "    ipv.plot([lb[0], lt[0]], [lb[1], lt[1]], [lb[2], lt[2]], color='blue')\n",
    "    ipv.scatter(np.array([x]), np.array([y]), np.array([z]), size=cam_sphere_size, marker=\"sphere\", color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images): \n",
    "    inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bed2f2",
   "metadata": {},
   "source": [
    "## Plot images in the camera wireframes\n",
    "To enhance the viewport visualization, let us\n",
    "* add the real-world images to the viewport \n",
    "* project the chessboard calibration points to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259de97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_picture(image, inv_extrinsic, vis_scale):\n",
    "    \"\"\" Plots a real-world image its respective 3D camera wireframe. \"\"\" \n",
    "    image = image.copy()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image = cv2.resize(image, None, fx=0.1, fy=0.1) / 255\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(-vis_cam_width/2 * vis_scale,  vis_cam_width/2 * vis_scale,  img_width), \n",
    "                         np.linspace(-vis_cam_height/2 * vis_scale, vis_cam_height/2 * vis_scale, img_height))\n",
    "    zz = np.ones_like(yy) * wire_frame_depth * vis_scale\n",
    "    coords = np.stack([xx, yy, zz, np.ones_like(zz)]) \n",
    "    coords = coords.reshape(4, -1) \n",
    "     \n",
    "    # Convert canera relative coordinates to world relative coordinates\n",
    "    coords = inv_extrinsic @ coords\n",
    "    xx, yy, zz, ones = coords.reshape(4, img_height, img_width) \n",
    "    ipv.plot_surface(xx, yy, zz, color=image)\n",
    "    \n",
    "    \n",
    "def project_points_to_picture(image, object_points, intrinsics, extrinsic):\n",
    "    \"\"\" Perspective projects points to an image and draws them green. \"\"\"\n",
    "    image = image.copy()\n",
    "    proj_matrix = intrinsics @ extrinsic\n",
    "    object_points = proj_matrix @ object_points\n",
    "    xs, ys, ones, disparity = object_points / object_points[2]\n",
    "    \n",
    "    for idx, (x, y) in enumerate(zip(xs, ys)):\n",
    "        x = round(x)\n",
    "        y = round(y)\n",
    "        if (0 < y < image.shape[0] and\n",
    "            0 < x < image.shape[1]):\n",
    "            # Each point occupies a 20x20 pixel area in the image.\n",
    "            image[y-10:y+10, x-10:x+10] = [0, 255, 0] \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851342e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images):\n",
    "    image = project_points_to_picture(image, object_points, intrinsics, extrinsic)\n",
    "    inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659cd46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Place bunny model in scene and project to cameras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68b71b",
   "metadata": {},
   "source": [
    "Now we can insert artificial 3D objects into the scene and project them to the cameras.    \n",
    "This is done for the infamous [Stanford bunny](https://www.cc.gatech.edu/~turk/bunny/bunny.html).     \n",
    "The bunny appears flat in the images because shading is not implemented and is out of this notebook's scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247c377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_bunny():\n",
    "    \"\"\"Plots the Stanford bunny pointcloud and returns its points. \n",
    "    \n",
    "    Returns: \n",
    "        bunny_coords (np.ndarray): a 4xn homogenous point cloud array.\n",
    "    \"\"\"\n",
    "    bunny_coords = np.load(open('data/bunny_point_cloud.npy', 'rb'))\n",
    "    b_xs, b_ys, b_zs = bunny_coords[:3]\n",
    "    bunny_point_size = 0.5\n",
    "    ipv.scatter(b_xs, b_ys, b_zs, size=bunny_point_size, marker=\"sphere\", color='lime')\n",
    "    return bunny_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "bunny_coords = plot_bunny() \n",
    "plot_chessboard(object_points)\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images):\n",
    "    image = project_points_to_picture(image, object_points, intrinsics, extrinsic)\n",
    "    image = project_points_to_picture(image, bunny_coords, intrinsics, extrinsic)\n",
    "    inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37fb25",
   "metadata": {},
   "source": [
    "## Plot camera optical axes\n",
    "By using the inverse of the projection matrix, $\\mathbf{P^{-1}}$, one can back-project the camera ray for a given image pixel.   \n",
    "In the following code, we back-project the center pixel of each camera. This should roughly equate to the camera's 'principal' or 'optical' axis.    \n",
    "Keep the reversed order of the extrinsics and intrinsics in mind.\n",
    "$$\\mathbf{P} = \\mathbf{KE}$$\n",
    "$$\\mathbf{P^{-1}} = \\mathbf{E^{-1} K^{-1}}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345db162",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "dir_vec_length = 50\n",
    "inv_intrinsics = np.linalg.inv(intrinsics)\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images):\n",
    "    image_center = np.array([image.shape[1]/2, image.shape[0]/2, 1, 1]).astype(int)\n",
    "\n",
    "    # Color the center of the image green.\n",
    "    x, y, _, _ = image_center\n",
    "    image = image.copy()\n",
    "    image[y-50:y+50, x-50:x+50] = [0, 255, 0]\n",
    "\n",
    "    inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "    dir_vec_in_cam_ref = inv_intrinsics @ image_center\n",
    "    \n",
    "    # Increase the length of the ray (only x,y,z not w).\n",
    "    # Scaling the whole homogenous vector would not change it.\n",
    "    dir_vec_in_cam_ref[:3] = dir_vec_in_cam_ref[:3] * dir_vec_length\n",
    "    dir_vec_in_world = inv_extrinsic @ dir_vec_in_cam_ref\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    cam_x, cam_y, cam_z, _ = cam_center\n",
    "    ipv.plot([cam_x, dir_vec_in_world[0]], [cam_y, dir_vec_in_world[1]], [cam_z, dir_vec_in_world[2]], color='red')\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37289bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = intrinsics[0,2]\n",
    "cy = intrinsics[1,2]\n",
    "print(f'Optical center according to the intrinsic matrix.\\n x:{cx:.1f} y:{cy:.1f}\\n')\n",
    "print(f'Optical center according to the images.\\n x:{x:.1f} y:{y:.1f} (assuming optical center is in middle)\\n')\n",
    "print(f'Discrepancy: \\n x:{abs(cx-x):.1f} y:{abs(cy-y):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c844a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot all camera rays towards the first inner corner of the chessboard\n",
    "Note that because we \n",
    "* keep track of the points 'disparity' after projection \n",
    "* use a full rank $4\\times4$ invertible projection matrix      \n",
    "\n",
    "we can back-project a camera ray with a length that reaches the appropriate world point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "chess_corner = np.array([0, 0, 0, 1])\n",
    "\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images):\n",
    "    proj_matrix = intrinsics @ extrinsic\n",
    "    chess_corner_proj = proj_matrix @ chess_corner\n",
    "    inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "    dir_vec_in_cam_ref = inv_intrinsics @ chess_corner_proj\n",
    "    dir_vec_in_world = inv_extrinsic @ dir_vec_in_cam_ref\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    cam_x, cam_y, cam_z, _ = cam_center\n",
    "    ipv.plot([cam_x, dir_vec_in_world[0]], [cam_y, dir_vec_in_world[1]], [cam_z, dir_vec_in_world[2]], color='red')\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d2efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Triangulate 3D points on the table with image point correspondences\n",
    "\n",
    "[Triangulation](https://en.wikipedia.org/wiki/Triangulation) in computer vision refers to the process of determining the location of a world point by finding near-intersections of camera rays. In this section we will:\n",
    "* find [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform) point correspondences in a pair of images.\n",
    "* back-project the image points into rays.\n",
    "* find the triangulated point where the rays intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5bbba",
   "metadata": {},
   "source": [
    "## Handpick correct SIFT correspondences\n",
    "Normally, one would use an automatic and robust method to find correspondences across multiple images. \n",
    "Due to the \n",
    "* large difference in camera poses\n",
    "* poor exposure of the images \n",
    "* absence of texture features on the table  \n",
    "\n",
    "valid point correspondences will be hand-picked in this code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_1_idx = 3\n",
    "camera_2_idx = 0 \n",
    "image_1 = images[camera_1_idx].copy()\n",
    "image_2 = images[camera_2_idx].copy()\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "keypoints_1, descriptors_1 = sift.detectAndCompute(image_1, None) # queryImage\n",
    "keypoints_2, descriptors_2 = sift.detectAndCompute(image_2, None) # trainimage\n",
    "\n",
    "# Match descriptors\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "# Apply ratio test \n",
    "good = []\n",
    "for best_match, second_best_match in matches:\n",
    "    if best_match.distance < 0.75 * second_best_match.distance:\n",
    "        good.append([best_match])\n",
    "\n",
    "# Sort matches according to descriptor distance\n",
    "dists = [g[0].distance for g in good]\n",
    "good = list(sorted(zip(dists, good)))\n",
    "good = [list(g) for g in zip(*good)][1]\n",
    "\n",
    "# Select manually validated matches\n",
    "hand_picked_matches = [2, 9, 15, 16, 18, 19, 22, 23, 24]\n",
    "good = np.array(good, dtype=object)[hand_picked_matches]\n",
    "\n",
    "# Plot matches \n",
    "match_image = cv2.drawMatchesKnn(image_1, keypoints_1, image_2, keypoints_2, good, None, [0, 255, 0])\n",
    "match_image = cv2.cvtColor(match_image, cv2.COLOR_RGB2BGR)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(match_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea71d49",
   "metadata": {},
   "source": [
    "## Encode and plot matches as image points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89275bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_coords_1 = list()\n",
    "match_coords_2 = list()\n",
    "\n",
    "for i in good:\n",
    "    i = i[0]\n",
    "    keypoint_1 = keypoints_1[i.queryIdx]\n",
    "    keypoint_2 = keypoints_2[i.trainIdx]\n",
    "    keypoint_1_center = np.array(keypoint_1.pt)\n",
    "    keypoint_2_center = np.array(keypoint_2.pt)\n",
    "    x1, y1 = keypoint_1_center\n",
    "    x2, y2 = keypoint_2_center\n",
    "    match_coords_1.append([x1, y1, 1, 1])\n",
    "    match_coords_2.append([x2, y2, 1, 1])\n",
    "    color = [0, 0, 255]\n",
    "    image1 = cv2.circle(image_1, keypoint_1_center.astype(int), 10, color, -1)\n",
    "    image2 = cv2.circle(image_2, keypoint_2_center.astype(int), 10, color, -1)\n",
    "\n",
    "match_coords_1 = np.array(match_coords_1)\n",
    "match_coords_2 = np.array(match_coords_2)\n",
    "\n",
    "image_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB)\n",
    "image_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image_concat = cv2.hconcat([image_1, image_2])\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image_concat)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b52aa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Draw back-projected rays of point correspondences\n",
    "Rays from the first camera are drawn in red, those from the second camera are drawn in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_1_idx = 3\n",
    "camera_2_idx = 0 \n",
    "\n",
    "init_3d_plot()\n",
    "plot_chessboard(object_points)\n",
    "\n",
    "for idx, (cam_center, extrinsic, image) in enumerate(zip(camera_centers, extrinsics, images)):\n",
    "    \n",
    "    if idx == camera_1_idx: \n",
    "        points = match_coords_1\n",
    "        color = 'red'\n",
    "    elif idx == camera_2_idx:\n",
    "        points = match_coords_2\n",
    "        color = 'lime'\n",
    "    else: \n",
    "        continue \n",
    "    \n",
    "    inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "    dir_vec_in_cam_ref = inv_intrinsics @ points.T\n",
    "    dir_vec_in_cam_ref[:3] *= 55\n",
    "    dir_vec_in_world = inv_extrinsic @ dir_vec_in_cam_ref\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    cam_x, cam_y, cam_z, _ = cam_center\n",
    "    \n",
    "    for vec in dir_vec_in_world.T:\n",
    "        ipv.plot([cam_x, vec[0]], [cam_y, vec[1]], [cam_z, vec[2]], color=color)\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8fbbc",
   "metadata": {},
   "source": [
    "# Triangulate, i.e. find the shortest line between two lines in 3D\n",
    "Given the \n",
    "* stereo point correspondences \n",
    "* intrinsic matrix of both cameras\n",
    "* extrinsic matrix of both cameras    \n",
    "\n",
    "pixel points can be back-projected into rays. The intersections of these corresponding rays are the triangulated 3D point.   \n",
    "The mathematics of finding this shortest line between two lines in 3D is detailed (with code examples) at http://paulbourke.net/geometry/pointlineplane/. \n",
    "We will take the midpoint of the shortest line joining two rays as the triangulated point, denoted here as $\\mathbf{m}$.\n",
    "\n",
    "![midpoint](./images/midpoint.gif)\n",
    "\n",
    "Here $P_{a}$ and $P_{b}$ are the desired closest points on lines $a$ and $b$ respectively.\n",
    "$$ P_{a} = P_{1} + \\mu_{a} (P2 - P1)$$\n",
    "$$ P_{b} = P_{3} + \\mu_{b} (P4 - P3)$$\n",
    "\n",
    "In our case, $P_{1}$ and $P_{3}$ are the first and second camera centers.\n",
    "$P_{2}$ and $P_{4}$ are the back-projected correspondence pixels in the first and second camera.\n",
    "  \n",
    "Solving for $\\mu_{a}$ and $\\mu_{b}$ provides the desired points on the lines. $\\mu_{a}$ and $\\mu_{b}$ have the following closed-form solution.\n",
    "Let:  \n",
    "$$P_{12} \\stackrel{\\text{def}}{=} P_{1} - P_{2} $$\n",
    "$$d_{1234} \\stackrel{\\text{def}}{=} P_{12} \\cdot P_{34} $$\n",
    "\n",
    "Then:  \n",
    "$$ \\mu_{a} = \\frac{d_{1343} d_{4321} - d_{1321} d_{4343}}{d_{2121} d_{4343} - d_{4321} d_{4321}}$$   \n",
    "\n",
    "$$ \\mu_{b} = \\frac{ d_{1343} + \\mu_{a} d_{4321}}  {d_{4343}}   $$\n",
    "\n",
    "See [this link](http://paulbourke.net/geometry/pointlineplane/) for the derivation. Note that the derivation is not complete, as it does not show the expansion of terms. This can be easily shown, however, by finding the reduced row echelon form of the equations that are derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc505b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def triangulate(p1, p2, p3, p4):\n",
    "    \"\"\" Calculates the point triangulated by two lines. \n",
    "        Also returns that points projection onto line 1 and 2, \n",
    "        know as Pa and Pb in the above description. \n",
    "    \"\"\"\n",
    "    # Strip potential scale factor of homogenous coord\n",
    "    p1 = p1[:3]\n",
    "    p2 = p2[:3]\n",
    "    p3 = p3[:3]\n",
    "    p4 = p4[:3]\n",
    "    \n",
    "    p13 = p1 - p3\n",
    "    p21 = p2 - p1\n",
    "    p43 = p4 - p3\n",
    "    \n",
    "    d1321 = np.dot(p13, p21)\n",
    "    d1343 = np.dot(p13, p43)\n",
    "    d2121 = np.dot(p21, p21)\n",
    "    d4321 = np.dot(p43, p21) \n",
    "    d4343 = np.dot(p43, p43) \n",
    "    \n",
    "    mu_a = (d1343 * d4321 - d1321 * d4343) / (d2121 * d4343 - d4321 * d4321)\n",
    "    mu_b = (d1343 + mu_a * d4321) / d4343\n",
    "    \n",
    "    point_on_line_1 = p1 + mu_a * p21\n",
    "    point_on_line_2 = p3 + mu_b * p43\n",
    "    \n",
    "    adjoining_line = point_on_line_2 - point_on_line_1\n",
    "    midpoint = adjoining_line / 2 \n",
    "    \n",
    "    triangulated_point = point_on_line_1 + midpoint\n",
    "    \n",
    "    return triangulated_point, point_on_line_1, point_on_line_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122f34b",
   "metadata": {},
   "source": [
    "## Plot triangulated points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18079b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_1 = images[camera_1_idx]\n",
    "image_2 = images[camera_2_idx] \n",
    "extrinsic_1 = extrinsics[camera_1_idx]\n",
    "extrinsic_2 = extrinsics[camera_2_idx]\n",
    "inv_extrinsic_1 = np.linalg.inv(extrinsic_1)\n",
    "inv_extrinsic_2 = np.linalg.inv(extrinsic_2)\n",
    "cam_center_1 = camera_centers[camera_1_idx]\n",
    "cam_center_2 = camera_centers[camera_2_idx]\n",
    "\n",
    "for coord_1, coord_2 in zip(match_coords_1, match_coords_2): \n",
    "    dir_vec_in_cam_ref_1 = inv_intrinsics @ coord_1\n",
    "    dir_vec_in_cam_ref_2 = inv_intrinsics @ coord_2\n",
    "    vec_1 = inv_extrinsic_1 @ dir_vec_in_cam_ref_1\n",
    "    vec_2 = inv_extrinsic_2 @ dir_vec_in_cam_ref_2\n",
    "    triangulated, _, _ = triangulate(cam_center_1, vec_1, cam_center_2, vec_2)\n",
    "    x, y, z = triangulated\n",
    "    print(f'x: {x:.1f},  \\ty: {y:.1f},  \\tz: {z:.1f}') \n",
    "    ipv.scatter(np.array([x]), np.array([y]), np.array([z]), marker=\"sphere\")\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b9764",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c3f50",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The triangulation was a success!    \n",
    "The rays align as expected, with the height of points on the table being consistently slightly under the chessboard.   \n",
    "This presents many possibilities. By knowing the real-world length of the chessboard, given enough point correspondences, we can:\n",
    "* infer the size and shape of the table, candles, and other objects in the scene.\n",
    "* insert objects into the scene and make them appear geometrically consistent in the cameras (as done with the bunny).\n",
    "* project patterns onto objects in a geometrically consistent manner (like dynamic selfie filters).\n",
    "\n",
    "While this method of triangulation is convenient for our current setup, it assumes cameras poses and point correspondences are given and correct. \n",
    "A more convenient method for determining 3D world points would allow:\n",
    "* iteratively refining camera poses and 3D world points simultaneously.\n",
    "* utilizing the point correspondences in more than 2 images for preciser and more robust triangulation.        \n",
    "\n",
    "Bundle adjustment provides these benefits and will be demonstrated in notebook 4.\n",
    "\n",
    "This concludes the first notebook.\n",
    "Having a visualization throughout each step in this notebook has made for a pleasant journey. Rather than taking the matrix manipulations and geometry theory at face value, we get to see and interact with it. Also, this can save quite some time when debugging. This is feasible thanks to [Ipyvolume](https://github.com/maartenbreddels/ipyvolume), which provides efficient OpenGL interfacing (unfortunately, Matplotlib doesn't support z-buffering in 3D mode).    \n",
    "\n",
    "Next up: a notebook on unit quaternions as 3D rotation representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1314fb",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80533a",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. [Szeliski, Richard. \"Computer vision: algorithms and applications.\" Springer Science & Business Media, 2010](https://szeliski.org/Book/)\n",
    "2. [Zisserman, Richard Hartley Andrew. \"Multiple view geometry in computer vision.\" 2004.](https://www.robots.ox.ac.uk/~vgg/hzbook/)\n",
    "3. [Zhang, Zhengyou. \"A flexible new technique for camera calibration.\" IEEE, 2000](https://ieeexplore.ieee.org/abstract/document/888718)\n",
    "4. [OpenCV Documentaiton](https://docs.opencv.org/4.5.4/) \n",
    "5. [Paul Bourke Geometry Website](http://paulbourke.net/geometry/pointlineplane/)\n",
    "6. [Ipyvolume by Maarten Breddels](https://github.com/maartenbreddels/ipyvolume) \n",
    "7. [Wikipedia](https://www.wikipedia.org/). Article links included in text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32502d7",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c2497",
   "metadata": {},
   "source": [
    "### Zhang's camera calibration algorithm\n",
    "Zhang's algorithm models camera intrinsics and extrinsics by: \n",
    "* Imposing constraints on the camera matrices by associating known world points on a calibration object with imaged pixel coordinates. \n",
    "* Imposing the constraint of a shared intrinsic matrix across all cameras.\n",
    "* Using an analytical solution to gain an initial estimate of each camera's pose and shared intrinsics. Distortions are not yet considered.\n",
    "* Refining the initial estimate with an iterative non-linear optimization of the reprojection error of the calibration points. This step also does not consider distortions.    \n",
    "    The Levenberg-Marquardt optimization algorithm is used. This algorithm iteratively \n",
    "    1. Computes the derivatives (Jacobian $J$) of the squared reprojection error w.r.t the camera parameters up to the second-order. Zhang used [Minpack](https://devernay.github.io/cminpack/), so numerical differentiation was used. For numerical differentiation, the second-order derivatives (Hessian $H$) are estimated using the [Jacobian product](https://math.stackexchange.com/questions/2349026/why-is-the-approximation-of-hessian-jtj-reasonable) $H = J^{T}J$.\n",
    "    2. Set the derivative of the squared error function to be zero. This is the analytical least-squares solution. That is, the reprojection error function is being modeled as a locally quadratic, ideally convex, function. This loss function will have the lowest loss where the derivative is zero, that is, where the loss lies in a valley, or a saddlepoint in the non-ideal case. This solution tends to miss or overshoot the optimal parameters when the initial guess of camera parameters is poor. This is due to the non-quadratic and even non-euclidean nature of the reprojection error w.r.t camera parameters. To solve this see step 3.\n",
    "    3. Determine a dampening factor $\\lambda$. This factor will determine the tradeoff between the analytical least-squares solution and the more stable but less optimal gradient-descent update for camera parameters. When $\\lambda=0$ only the analytical least-squares solution is used. This is known as [Guass-Newton optimization](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm). For a greater $\\lambda$, the update is closer to a gradient-descent step. This is known as [Levenberg-Marquardt optimization](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm).\n",
    "    4. Update the camera parameters. Observe the new points projections and reprojection errors. Iterate until convergence.\n",
    "* Finally, estimate radial distortion by another round of Levenberg-Marquardt minimization of the reprojection error. That is, introduce distortion parameters into the image formation model and reoptimize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
